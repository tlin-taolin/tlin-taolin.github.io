
<html>

<head>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <link rel="stylesheet" type="text/css" href="style.css" />
    <title>tlin</title>
    <base href="https://tlin-tao-lin.github.io/project.html">
</head>

<body>
<h1 style="padding-left: 0.5em">Tao LIN</h1><hr>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
    <div class="menu-item"><a href="index.html">Home</a></div>
    <div class="menu-item"><a href="publication.html">Publications</a></div>
    <div class="menu-item"><a href="misc.html">Miscellaneous</a></div>
</td>

<td id="layout-content">

    <h1 style="margin-top: 0em">Selected Projects </h1><br>
    <p>
    * indicates equal contribution
    </p>

    <h2>Understanding Deep Learning for Efficient and Effective Distributed Training</h2>
    <div class="infoblock">
    <div class="blockcontent">
    <p>
        This project aims to 
        (i) better understand the geometry/trajactory of Deep Learning training from both empirical and theoretical aspects, 
        and (ii) develop efficient and effective distributed algorithms for deep learning.
    </p>
    <p><b>Related Papers</b>:</p>
    <ul>
        <li><p>
            "Extrapolation for Large-batch Training in Deep Learning".
            <b>Tao Lin*</b>, Lingjing Kong*, Sebastian U. Stich, Martin Jaggi.
            ICML, 2020.
            [ <a href="https://arxiv.org/abs/2006.05720">paper</a> ]
        </p></li>

        <li><p>
            "Decentralized Deep Learning with Arbitrary Communication Compression".
            Anastasia Koloskova*, <b>Tao Lin*</b>, Sebastian U. Stich, Martin Jaggi.
            ICLR, 2020.
            [ <a href="https://openreview.net/forum?id=SkgGCkrKvH">paper</a> ]
        </p></li>

        <li><p>
            "Don't Use Large Mini-Batches, Use Local SGD".
            <b>Tao Lin</b>, Sebastian U. Stich, Kumar Kshitij Patel, Martin Jaggi.
            ICLR, 2020.
            [ <a href="https://openreview.net/forum?id=B1eyO1BFPr">paper</a> ]
        </p></li>
    </ul>
    </div>
    </div>

    <h2>Efficient (Centralized/Decentralized/Federated) Training and Inference for Edge Devices</h2>
    <div class="infoblock">
    <div class="blockcontent">
    <p>
        This project aims to leverage the theoretical tools
        to design better practical learning/inference algorithms for Edge AI.
    </p>
    <p><b>Related Papers</b>:</p>
    <ul>
        <li><p>
            "Ensemble Distillation for Robust Model Fusion in Federated Learning".
            <b>Tao Lin*</b>, Lingjing Kong*, Sebastian U. Stich, Martin Jaggi.
            NeurIPS, 2020.
            [ <a href="https://arxiv.org/abs/2006.07242">paper</a> ]
        </p></li>

        <li><p>
            "Masking as an Efficient Alternative to Finetuning for Pretrained Language Models".
            Mengjie Zhao*, <b>Tao Lin*</b>, Fei Mi, Martin Jaggi, Hinrich Schütze.
            EMNLP, 2020. 
            [ <a href="https://arxiv.org/abs/2004.12406">long paper</a> ]
        </p></li>

        <li><p>
            "Decentralized Deep Learning with Arbitrary Communication Compression".
            Anastasia Koloskova*, <b>Tao Lin*</b>, Sebastian U. Stich, Martin Jaggi.
            ICLR, 2020.
            [ <a href="https://openreview.net/forum?id=SkgGCkrKvH">paper</a> ]
        </p></li>

        <li><p>
            "Dynamic Model Pruning with Feedback".
            <b>Tao Lin</b>, Sebastian U. Stich, Luis Barba, Daniil Dmitriev, Martin Jaggi.
            ICLR, 2020.
            [ <a href="https://openreview.net/forum?id=SJem8lSFwB">paper</a> ]
        </p></li>

        <li><p>
            "Training DNNs with Hybrid Block Floating Point".
            Mario Drumond, <b>Tao Lin</b>, Martin Jaggi, Babak Falsafi.
            NeurIPS, 2018.
            [ <a href="https://arxiv.org/abs/1804.01526">paper</a> ]
        </p></li>
    </ul>
    </div>
    </div>

    <!--
        <h2>Continual and/or Personalized Learning on Decentralized/Federated Edge Devices</h2>
        <div class="infoblock">
        <div class="blockcontent">
        <p>
            This project looks into practical IoT scenarios with EdgeAI,
            where learning is performed in a continual/streaming way on non-i.i.d. (personalized) data from edge devices.
        </p>
        <p><b>Related Papers</b>:</p>
        <ul>
            <li><p>
                "Generalized Class Incremental Learning".              
                Fei Mi*, Lingjing Kong*, <b>Tao Lin</b>, Kaicheng Yu, Boi Faltings.
                CVPR workshop, 2020.
                [ <a href="http://openaccess.thecvf.com/content_CVPRW_2020/html/w15/Mi_Generalized_Class_Incremental_Learning_CVPRW_2020_paper.html">paper</a> ]
            </p></li>
        </ul>
        </div>
        </div>

        <h2>(Distributed) Deep Learning with Robustness (adversarial/byzantine)</h2>
        <div class="infoblock">
        <div class="blockcontent">
        <p>
            This project aims to (i) understand the robustness in deep learning, 
            and (ii) propose robust (distributed) algorithms to against attacks (e.g. adversarial, poisoning).
        </p>
        <p><b>Related Papers</b>:</p>
        <ul>
            <li><p>
                "On the Loss Landscape of Adversarial Training: Identifying Challenges and How to Overcome Them".
                Chen Liu, Mathieu Salzmann, <b>Tao Lin</b>, Ryota Tomioka, Sabine Süsstrunk.
                NeurIPS, 2020
                [ <a href="https://arxiv.org/abs/2006.08403">paper</a> ]
            </p></li>
        </ul>
        </div>
        </div>
    -->

</td>
</tr>
</table>
</body>
</html>
