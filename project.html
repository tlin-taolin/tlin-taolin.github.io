
<html>

<head>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <link rel="stylesheet" type="text/css" href="style.css" />
    <title>tlin</title>
    <base href="https://tlin-tao-lin.github.io/publication.html">
</head>

<body>
<h1 style="padding-left: 0.5em">Tao LIN</h1><hr>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
    <div class="menu-item"><a href="index.html">Home</a></div>
    <div class="menu-item"><a href="publication.html" class="current">Publications</a></div>
    <div class="menu-item"><a href="project.html" class="current">Projects</a></div>
    <div class="menu-item"><a href="misc.html">Miscellaneous</a></div>
</td>

<td id="layout-content">

    <h1 style="margin-top: 0em">Selected Projects </h1><br>
    <p>
    * indicates equal contribution<br>
    </p>

    <h2>Understanding Deep Learning for Efficient and Effective Distributed Training</h2>
    <div class="infoblock">
    <div class="blockcontent">
    <p>
        This project aims to 
        (i) better understand the geometry/trajactory of Deep Learning training from both empirical and theoretical aspects, 
        and (ii) develop efficient and effective distributed algorithms for deep learning.
    </p>
    <p><b>Related Papers</b>:</p>
    <ul>
        <li><p>
            "Extrapolation for Large-batch Training in Deep Learning".
            <b>Tao Lin*</b>, Lingjing Kong*, Sebastian U. Stich, Martin Jaggi.
            <i>Thirty-seventh International Conference on Machine Learning (ICML)</i>, 2020.
            [ <a href="https://arxiv.org/abs/2006.05720">paper</a> ]
        </p></li>

        <li><p>
            "Decentralized Deep Learning with Arbitrary Communication Compression".
            Anastasia Koloskova*, <b>Tao Lin*</b>, Sebastian U. Stich, Martin Jaggi.
            <i>International Conference on Learning Representations (ICLR)</i>, 2020.
            [ <a href="https://openreview.net/forum?id=SkgGCkrKvH">paper</a> ]
        </p></li>

        <li><p>
            "Don't Use Large Mini-Batches, Use Local SGD".
            <b>Tao Lin</b>, Sebastian U. Stich, Kumar Kshitij Patel, Martin Jaggi.
            <i>International Conference on Learning Representations (ICLR)</i>, 2020.
            [ <a href="https://openreview.net/forum?id=B1eyO1BFPr">paper</a> ]
        </p></li>
    </ul>
    </div>
    </div>

    <h2>Efficient (Centralized/Decentralized/Federated) Training and Inference for Edge Devices</h2>
    <div class="infoblock">
    <div class="blockcontent">
    <p>
        This project aims to leverage the theoretical tools
        to design better practical learning/inference algorithms for Edge AI.
    </p>
    <p><b>Related Papers</b>:</p>
    <ul>
        <li><p>
            "Masking as an Efficient Alternative to Finetuning for Pretrained Language Models".
            Mengjie Zhao*, <b>Tao Lin*</b>, Martin Jaggi, Hinrich Schütze.
            Arxiv preprint, 2020.
            [ <a href="https://arxiv.org/abs/2004.12406">paper</a> ]
        </p></li>

        <li><p>
            "Decentralized Deep Learning with Arbitrary Communication Compression".
            Anastasia Koloskova*, <b>Tao Lin*</b>, Sebastian U. Stich, Martin Jaggi.
            <i>International Conference on Learning Representations (ICLR)</i>, 2020.
            [ <a href="https://openreview.net/forum?id=SkgGCkrKvH">paper</a> ]
        </p></li>

        <li><p>
            "Dynamic Model Pruning with Feedback".
            <b>Tao Lin</b>, Sebastian U. Stich, Luis Barba, Daniil Dmitriev, Martin Jaggi.<br>
            <i>International Conference on Learning Representations (ICLR)</i>, 2020.
            [ <a href="https://openreview.net/forum?id=SJem8lSFwB">paper</a> ]
        </p></li>

        <li><p>
            "Training DNNs with Hybrid Block Floating Point".
            Mario Drumond, <b>Tao Lin</b>, Martin Jaggi, Babak Falsafi.
            <i>Thirty-second Conference on Neural Information Processing Systems (NeurIPS)</i>, 2018.
            [ <a href="https://arxiv.org/abs/1804.01526">paper</a> ]
        </p></li>
    </ul>
    </div>
    </div>

    <h2>Continual and/or Personalized Learning on Decentralized/Federated Edge Devices</h2>
    <div class="infoblock">
    <div class="blockcontent">
    <p>
        This project looks into practical IoT scenarios with EdgeAI,
        where learning is performed in a continual/streaming way on non-i.i.d. (personalized) data from edge devices.
    </p>
    <p><b>Related Papers</b>:</p>
    <ul>
        <li><p>
            "Generalized Class Incremental Learning".              
            Fei Mi*, Lingjing Kong*, <b>Tao Lin</b>, Kaicheng Yu, Boi Faltings.
            <i>The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 2020 (workshop track).
            [ <a href="http://openaccess.thecvf.com/content_CVPRW_2020/html/w15/Mi_Generalized_Class_Incremental_Learning_CVPRW_2020_paper.html">paper</a> ]
        </p></li>

        <li><p>
            "Memory-based Hebbian Parameter Adaptation". Fei Mi, <b>Tao Lin</b>, Boi Faltings. [ paper ]
        </p></li>
    </ul>
    </div>
    </div>

    <h2>(Distributed) Deep Learning with Robustness</h2>
    <div class="infoblock">
    <div class="blockcontent">
    <p>
        This project aims to (i) understand the robustness in deep learning, 
        and (ii) propose robust (distributed) algorithms to against attacks (e.g. adversarial, poisoning).
    </p>
    <p><b>Related Papers</b>:</p>
    <ul>
        <li><p>
            "On the Loss Landscape of Adversarial Training: Identifying Challenges and How to Overcome Them".
            Chen Liu, Mathieu Salzmann, <b>Tao Lin</b>, Ryota Tomioka, Sabine Süsstrunk.
            Arxiv preprint, 2020.
            [ <a href="https://arxiv.org/abs/2006.08403">paper</a> ]
        </p></li>
    </ul>
    </div>
    </div>

</td>
</tr>
</table>
</body>
</html>
